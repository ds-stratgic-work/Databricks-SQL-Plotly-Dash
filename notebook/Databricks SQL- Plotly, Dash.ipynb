{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9648a3a9-d241-49cf-aea4-39356df2868b",
   "metadata": {},
   "source": [
    "# Databricks SQL- Plotly, Dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebc309-810d-4880-96ac-bbf1d6cb11c4",
   "metadata": {},
   "source": [
    "# Table Of Contents\n",
    "[1. Introduction](#1.Introduction)\\\n",
    "[2. Why Plotly Resampler?](#2.Why_Plotly_Resampler? )\\\n",
    "[3. Prerequisites](#3.-Prerequisites)\\\n",
    "[4. High-Level Steps](#4.-High_Level_Steps)\\\n",
    "[5. High-Level Procedure](#4.-High_Level_Procedure)\\\n",
    "[6. Installation of Databricks SQL Connector for Python](#4.-Installation_of_Databrick_SQL_Connector_for_Python)\\\n",
    "[7. Having No time-series dataset](#7.-Having_No_time-series_dataset)\\\n",
    "[8. Query to SQL Wearhouse](#8.-Query_to_SQL_Wearhouse)\\\n",
    "[9. Plotly Resampler](#9.-Plotly_Resampler)\\\n",
    "[10. Raising the Stakes with Polars](#10.-Raising_the_Stakes_with_Polars)\\\n",
    "[11. Polars, a substitute for Pandas](#11.-Polars_a_substitute_for_Pandas)\\\n",
    "[12. The Datashader Alternative](#12.-The_Datashader_Alternative)\\\n",
    "[13. Conclusion](#13.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786bb67-1d54-4486-8d65-0941a9c07ed5",
   "metadata": {},
   "source": [
    "### 1. Introduction \n",
    "\n",
    "This article aims to provide a thorough overview of our journey in visualizing vast time-series data stored in Databricks by utilizing advanced `aggregation and downsampling` techniques from the Plotly Resampler. Following this, we will demonstrate how these technologies are combined to create a production data application powered by Dash, optimized for `Polars`, and utilizing Plotly.\n",
    "\n",
    "This approach allows for a more in-depth exploration of underlying patterns and trends and provides a `scalable solution` for handling and interpreting data on an industrial level. Throughout this process, we will address the challenges associated with working with large time-series datasets and provide an overview of the requirements for querying and visualizing 10¹⁰ individual data points at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc4b43-5e9c-4a94-8d7a-646d0bf036e2",
   "metadata": {},
   "source": [
    "### 2. Why Plotly Resampler?\n",
    "The Plotly Resampler is an extension of Plotly’s Python bindings, improving the scalability of line charts within an interactive toolkit by consolidating the underlying data based on the current graph view. This can significantly enhance your visualization of extensive sets of sequential data.\n",
    "\n",
    "- Performance improvement for extensive datasets: The Plotly chart interactivity is optimized for datasets with 100,000 data points or less by default. However, for large-scale time-series data, the Plotly Resampler dynamically aggregates data based on the current scale, resulting in smoother interactions and faster load times.\n",
    "- Interactive data manipulation: The tool uses callbacks to update and aggregate data as users interact with the plot, making exploring large datasets more intuitive and responsive, especially when zooming or panning.\n",
    "- Flexible aggregation methods: It provides interfaces for various sequence aggregation techniques, allowing users to select or develop an algorithm that best suits their data visualization requirements.\n",
    "- Enhanced user interface: The Resampler modifies the double-click behavior within a line-chart area to initiate an \"Autoscale\" event, improving the overall user experience.\n",
    "\n",
    "\n",
    "### 3. Prerequisites\n",
    "This article is intended for individuals with substantial amounts of time-series data stored in either ADLS2 Blob Storage or AWS S3 Buckets, seeking visualization and exploration methods for their data. Suppose you do not have your data. In that case, you can utilize the Raspberry Pi Azure IoT Online Simulator to stream simulated data into Blob Storage (as detailed in Article #2) or use the provided CSV in the repository to upload to Databricks and write to Delta tables using the accompanying SQL or PySpark module.\n",
    "\n",
    "\n",
    "\n",
    "### 4. High-Level Steps\n",
    "Performance improvement for extensive datasets: Typically, Plotly chart interactivity is most effective when dealing with datasets of 100,000 data points or less. When working with large-scale time-series data and aiming to interactively engage with all the data, the Plotly Resampler dynamically consolidates data based on the current scale, ensuring smoother interactions. Apart from a Delta table containing a time-series dataset, the following are required:\n",
    "\n",
    "1. A Databricks workspace with Databricks SQL enabled (DB SQL is enabled by default in Premium Workspaces or above)\n",
    "2. A DB SQL endpoint or Databricks cluster with 9.1 LTS or higher (data engineering cluster)\n",
    "3. A personal access token in Databricks to authenticate SQL Endpoint via API\n",
    "4. A Python development environment (>=v 3.10). It is recommended to use VSCode for a local IDE and utilize Conda or virtual env to manage dependencies, along with `black` for automatic code formatting.\n",
    "\n",
    "### 5. High-Level Procedure\n",
    "For the context of this document, it is assumed that the reader already possesses time-series data stored in the cloud or has utilized one of the data generation techniques outlined in the prerequisite section to generate a dataset. The subsequent steps undertaken were as follows:\n",
    "\n",
    "1. Employ the Medallion Architecture to establish a pipeline that acquires raw IoT data from the cloud/source and writes to three stages of Delta\n",
    "2. Utilize the Databricks SQL Connector for Python to query the Delta table on the SQL Warehouse\n",
    "3. Channel data from SQL Warehouse into Arrow files (opt for Parquet if replication is desired) residing on the persistent file storage system provided with Dash Enterprise (alternatively, they can be stored in the root directory of the project folder at the expense of memory)\n",
    "4. Import data from Arrow files into a Polars-optimized version of the Plotly Resampler for visualization (or import parquet files into the standard Plotly Resample)Performance improvement for extensive datasets: Typically, Plotly chart interactivity is most effective when dealing with datasets of 100,000 data points or less. When working with large-scale time-series data and aiming to interactively engage with all the data, the Plotly Resampler dynamically consolidates data based on the current scale, ensuring smoother interactions. Apart from a Delta table containing a time-series dataset, the following are required:\n",
    "\n",
    "1. A Databricks workspace with Databricks SQL enabled (DB SQL is enabled by default in Premium Workspaces or above)\n",
    "2. A DB SQL endpoint or Databricks cluster with 9.1 LTS or higher (data engineering cluster)\n",
    "3. A personal access token in Databricks to authenticate SQL Endpoint via API\n",
    "4. A Python development environment (>=v 3.10). It is recommended to use VSCode for a local IDE and utilize Conda or virtual env to manage dependencies, along with black for automatic code formatting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5eece-bb76-43b2-ae22-792fd86267d7",
   "metadata": {},
   "source": [
    "### 6.Installation of Databricks SQL Connector for Python\n",
    "\n",
    "!pip install databricks-sql-connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f74e2-7b1a-4279-ae66-3bd494a16df8",
   "metadata": {},
   "source": [
    "### 7. Having No time-series dataset\n",
    "If you do not possess a substantial time-series dataset for visualization purposes, this section is designed specifically for you. In case you have already stored your measurement data in Delta tables, feel free to proceed directly to Step 2: Query SQL Warehouse. \n",
    "\n",
    "In the context of an optimal industrial IoT pipeline, it is recommended to implement the Medallion Architecture for preprocessing data as illustrated in the diagram.\n",
    "\n",
    "\n",
    "\n",
    "It is advisable to consider the CSV produced by the Python script as a valuable layer when referring to this article. After uploading the CSV to your Databricks File System, you have the option to transfer its data to a Delta table using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b0954-0c1e-4c5c-bd35-6a3e84ef147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sample code using PySpark\n",
    "\n",
    "%python\n",
    "# File location and type\n",
    "file_location = \"/FileStore/auto_iot_sensor_data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)\n",
    "\n",
    "permanent_table_name = \"main.resamplerdata.auto_iot_data_bronze_sensors\"\n",
    "df.write.format(\"delta\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1e94e-7945-4192-bbb3-9f3803fa00cb",
   "metadata": {},
   "source": [
    "### 8. Query to SQL Wearhouse\n",
    "\n",
    "Once the pipeline construction is completed and data starts flowing into the Delta tables, the subsequent action involves utilizing the Databricks SQL Connector for Python to retrieve data from the SQL Warehouse and save the query results as a Parquet file in our Dash application. The provided code outlines the process of establishing a connection engine through SQLAlchemy using your Databricks credentials.\n",
    "\n",
    "Once the pipeline construction is completed and data starts to be inserted into the Delta tables, the subsequent task involves utilizing the Databricks SQL Connector for Python to execute a query on the SQL Warehouse and retrieve the query results as a Parquet file within our Dash application. The provided code outlines the process of establishing a connection engine using SQLAlchemy and your Databricks credentials. If you are working through the GitHub repository, simply include your Databricks credentials in the .env file located in the project folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5782932-e20e-443d-8786-ff0349f7e209",
   "metadata": {},
   "source": [
    "#### Installation \n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4cebd-da68-4049-9c9b-3d7ea7285dad",
   "metadata": {},
   "source": [
    "1. A demonstration of how to extract information from Databricks using the SQLAlchemy engine and save it into Parquet files.\r\n",
    "2. An example of extracting data from Databricks usingthe  SQLAlchemy engine and then storing it in Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d26ee-2097-44f2-b730-11129d7a5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine_url = f\"databricks://token:{token}@{host}/?http_path={path}&catalog=main&schema=information_schema\"\n",
    "engine = create_engine(engine_url)\n",
    "\n",
    "# # Fetch data from the database\n",
    "engine_temp_stmt = \"SELECT Timestamp, EngineTemperature_C FROM main.resamplerdata.auto_iot_bronze_sensors ORDER BY Timestamp ASC LIMIT 10000000;\"\n",
    "oil_pressure_stmt = \"SELECT Timestamp, OilPressure_psi FROM main.resamplerdata.auto_iot_bronze_sensors ORDER BY Timestamp ASC LIMIT 10000000;\"\n",
    "tire_pressure_stmt = \"SELECT Timestamp, TirePressure_psi FROM main.resamplerdata.auto_iot_bronze_sensors ORDER BY Timestamp ASC LIMIT 10000000;\"\n",
    "battery_voltage_stmt = \"SELECT Timestamp, BatteryVoltage_V FROM main.resamplerdata.auto_iot_bronze_sensors ORDER BY Timestamp ASC LIMIT 10000000;\"\n",
    "\n",
    "\n",
    "engine_temp_df = pd.read_sql_query(engine_temp_stmt, engine)\n",
    "oil_pressure_df = pd.read_sql_query(oil_pressure_stmt, engine)\n",
    "speed_df = pd.read_sql_query(speed_stmt, engine)\n",
    "tire_pressure_df = pd.read_sql_query(tire_pressure_stmt, engine)\n",
    "battery_voltage_df = pd.read_sql_query(battery_voltage_stmt, engine)\n",
    "\n",
    "engine_temp_df.to_parquet(f\"backend-data/engine_temp_df.parquet\")\n",
    "oil_pressure_df.to_parquet(f\"backend-data/oil_pressure_df.parquet\")\n",
    "speed_df.to_parquet(f\"backend-data/speed_df.parquet\")\n",
    "tire_pressure_df.to_parquet(f\"backend-data/tire_pressure_df.parquet\")\n",
    "battery_voltage_df.to_parquet(f\"backend-data/battery_voltage_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d17e39-1b4d-4bad-a615-bbc2b40ec564",
   "metadata": {},
   "source": [
    "The information is saved in arrow files within a persistent file storage system provided by Dash Enterprise. The example above illustrates the process of saving data from Databricks into Parquet files located in a folder named back-end data within our project. Here, the data is partitioned and stored based on columns. By caching the queried data in this manner, unnecessary data transfers over the network are avoided. Additionally, the application can be configured to only process new data at scheduled intervals, reducing bandwidth usage, improving performance, and optimizing resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc397f4-febf-4930-a00d-53bf12107932",
   "metadata": {},
   "source": [
    "### 9.Plotly Resampler\n",
    "Once the data has been retrieved and stored, we can proceed with loading our Arrow files into the Resampler for visualization. Following the data extraction from Databricks, it is saved as Arrow files in our persistent file system. The Resampler utilizes Dash callbacks to register inputs and dynamically update the figure, as illustrated in the code below.\n",
    "\n",
    "\n",
    "\n",
    "#### Callbacks demonstrating Resampler logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddcc774-dc66-4ec5-8593-411fed6476ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- graph construction logic + callback ---------\n",
    "@app.callback(\n",
    "    [\n",
    "        Output(\"coarse-graph\", \"figure\"),\n",
    "        Output(\"plotly-resampler-graph\", \"figure\"),\n",
    "        ServersideOutput(\"store\", \"data\"),\n",
    "    ],\n",
    "    [Input(\"plot-button\", \"n_clicks\"), *get_selector_states(len(name_folder_list))],\n",
    "    prevent_initial_call=True,\n",
    ")\n",
    "def construct_plot_graph(n_clicks, *folder_list):\n",
    "    it = iter(folder_list)\n",
    "    file_list: List[Path] = []\n",
    "    for folder, files in zip(it, it):\n",
    "        if not all((folder, files)):\n",
    "            continue\n",
    "        else:\n",
    "            files = [files] if not isinstance(files, list) else file_list\n",
    "            for file in files:\n",
    "                file_list.append((Path(folder).joinpath(file)))\n",
    "\n",
    "    ctx = callback_context\n",
    "    if len(ctx.triggered) and \"plot-button\" in ctx.triggered[0][\"prop_id\"]:\n",
    "        if len(file_list):\n",
    "            # Create two graphs, a dynamic plotly-resampler graph and a coarse graph\n",
    "            dynamic_fig: FigureResampler = visualize_multiple_files(file_list)\n",
    "            coarse_fig: go.Figure = go.Figure(\n",
    "                FigureResampler(dynamic_fig, default_n_shown_samples=3_000)\n",
    "            )\n",
    "\n",
    "            coarse_fig.update_layout(title=\"<b>coarse view</b>\", height=250)\n",
    "            coarse_fig.update_layout(margin=dict(l=0, r=0, b=0, t=40, pad=10))\n",
    "            coarse_fig.update_layout(showlegend=False)\n",
    "            coarse_fig._config = coarse_fig._config.update(\n",
    "                {\"modeBarButtonsToAdd\": [\"drawrect\"]}\n",
    "            )\n",
    "\n",
    "            dynamic_fig._global_n_shown_samples = 1000\n",
    "            dynamic_fig.update_layout(title=\"<b>dynamic view<b>\", height=450)\n",
    "            dynamic_fig.update_layout(margin=dict(l=0, r=0, b=40, t=40, pad=10))\n",
    "            dynamic_fig.update_layout(\n",
    "                legend=dict(\n",
    "                    orientation=\"h\", y=-0.11, xanchor=\"right\", x=1, font_size=18\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return coarse_fig, dynamic_fig, dynamic_fig\n",
    "    else:\n",
    "        return no_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22543406-37f0-46d6-b119-98c980217b6b",
   "metadata": {},
   "source": [
    "There are various methods to utilize and further personalize the Plotly Resampler. In this demonstration, we are showcasing a page that displays a simultaneous coarse and fine-grained view, a graph that supports multiple overlaying y-axes, and a page that allows users to add traces for comparison. A significant feature of the Plotly Resampler is its capability to update the graph based on user input. For instance, modifications to the coarse graph will automatically trigger updates to the fine-grained graph, thereby adjusting the view accordingly. Moreover, in this article, we are utilizing a Polars dataframe with the Plotly Resampler to enhance performance compared to pandas. Stay tuned for a future article that will showcase zero MB visualizations using Polars and Plotly Resampler!\n",
    "\n",
    "\n",
    "Similar to all Dash charts, we can fully personalize the colors, themes, line sizes, and legend visibility to enhance readability and aesthetics. By utilizing the Resampler, we can efficiently navigate through a dataset encompassing 6 months of time-series data, handling up to 50 million rows (or more based on memory capacity), and zoom into the sub-second level with minimal latency. Additionally, it supports the integration of multiple y-axes, encouraging a comprehensive cross-analysis of insights.\n",
    "\n",
    "\n",
    "The Plotly Resampler performs exceptionally well for datasets containing around 150 million to 200 million points. However, when dealing with larger datasets, memory constraints become an issue. This is where Polars comes in. By using Polars dataframes instead of Pandas inside the Resampler library, we were able to significantly reduce the resources consumed, allowing us to visualize datasets with as many as 1 billion points. Polars achieves this through lazy evaluation, Arrow memory layout, and built-in multi-threading, making it a more efficient alternative to Pandas, especially for large datasets. These performance enhancements are critical for the Plotly Resampler, enabling users to process and resample large datasets quickly and efficiently without the need for additional hardware or complex optimizations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f482273-a82c-49b1-ab15-81ac69b039ec",
   "metadata": {},
   "source": [
    "### 10. Raising the Stakes with Polars\n",
    "Transitioning to Polars within the Resampler library has not only enabled the handling of larger datasets but has also improved processing speed for current ones. Users are now able to enjoy faster performance without sacrificing scale. The Polars Plotly Resampler library is a continuously evolving project at Plotly, with additional improvements on the horizon.\n",
    "\n",
    "\n",
    "\n",
    "### 11.  Polars, a substitute for Pandas\n",
    "Polars, a substitute for Pandas, is specifically designed for high-performance, multi-threaded data processing. There are several reasons why Polars surpasses Pandas, particularly for extensive datasets:\n",
    "\n",
    "1. Lazy evaluation: Polarutilizees lazy evaluation, meaning computations are not immediately executed when an operation is called. Instead, they are queued up and executed in a single pass. This optimizes the sequence of operations and reduces unnecessary intermediate calculations.\n",
    "\n",
    "2. Arrow memory layout: Polars utilizes Apache Arrow for its memory layout. Arrow provides columnar memory, ensuring that data is stored contiguously in memory, resulting in faster processing times.\n",
    "\n",
    "3. Built-in multi-threading: While Pandas may require manual parallelization for optimal performance, Polars automatically leverages multi-threading, maximizing CPU utilization for data operations.\n",
    "\n",
    "In the context of the Plotly Resampler, these performance improvements are crucial. When working with large datasets, the ability to efficiently process and resample data is essential. With the memory efficiencies of Polars, users can now handle datasets that were previously inaccessible without the need for additional hardware or complex optimization.\n",
    "\n",
    "\n",
    " Utilizing Polars within the Resampler library not only enabled the handling of larger datasets but also improved the processing speed for current ones. Users are no longer required to sacrifice speed or scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c33000-a6ce-4769-860f-8748f18beab6",
   "metadata": {},
   "source": [
    "### 12. The Datashader Alternative\n",
    "Datashader’s functionality is further enhanced by its seamless integration with the revered visualization library, Bokeh, allowing for interactive plots that can captivate viewers. In the realm of geospatial data visualization, where large datasets are common and downsampling of 2D data representations is necessary, Datashader proves to be a reliable tool. However, it does have its drawbacks. For beginners, especially those unfamiliar with Python scripting, navigating Datashader can be challenging, while the Plotly Resampler can be used as a wrapper for a Plotly figure. Additionally, when used on its own, Datashader tends to offer less interactivity compared to other tools like Plotly. These limitations, along with the potential loss of finer data details in rasterized outputs, highlight areas where Datashader can be improved. (Sidenote: We are exploring ways to integrate Datashader with Plotly charts / Dash to simplify the process and make this capability more accessible to our enterprise audience)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111064b9-23f7-4376-a8cd-d074ffbd76ab",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "While working with large-scale time-series data visualization, the Plotly Resampler's functionalities truly work out, especially when paired with Databricks ecosystems. It's not just the capacity to manage extensive datasets that sets it apart, but also its distinctive interactive features. These, combined with extensive customization choices and significantly improved performance, establish it as an essential tool for individuals working with expansive time-series data.\n",
    "\n",
    "Although Datashader is praiseworthy, particularly for its innovative rasterization methods and integration with Bokeh, it may not offer the same level of user-friendly interactivity as the Plotly Resampler. This distinction makes the latter the preferred option for industrial IoT time-series scenarios.\n",
    "\n",
    "Moreover, the incorporation of Polars into the Resampler's workflow enhances the rapid advancement and flexibility of the data science toolkit. \n",
    "\n",
    "In conclusion, the combined capabilities of Plotly's charting library, Dash, Dash Enterprise, the Plotly Resampler, and Databricks provide an unmatched toolkit, enabling professionals to delve deeper into vast datasets and redefine the boundaries of data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829024b-e8b4-4396-9761-aaa554797e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
